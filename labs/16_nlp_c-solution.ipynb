{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stop words, tf-idf\n",
    "\n",
    "Let's investigate one of the most useful feature weightings, and how stop words derive naturally from that. To start, let's load a set of small documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "try:\n",
    "    df = pd.read_csv('data/rt_critics.csv')\n",
    "except IOError:\n",
    "    print 'cannot find file'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['So ingenious in concept, design and execution that you could watch it on a postage stamp-sized screen and still be engulfed by its charm.',\n",
       " \"The year's most inventive comedy.\",\n",
       " 'A winning animated feature that has something for everyone on the age spectrum.',\n",
       " \"The film sports a provocative and appealing story that's every bit the equal of this technical achievement.\",\n",
       " \"An entertaining computer-generated, hyperrealist animation feature (1995) that's also in effect a toy catalog.\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It seems silly to call such short blurbs 'documents', but we'll stick with the NLP nomenclature.\n",
    "\n",
    "documents = list(df['quote'])\n",
    "documents[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Frequency\n",
    "\n",
    "Let's start by calculating the document frequency for words in these documents. For this task, let's also remove all the punctuation marks and make everything lower-case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize  # for tokenizing our text\n",
    "import string  # helps with removing punctuation\n",
    "from collections import Counter  # great dict-like datastructure for counting things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a few tokens: [['so', 'ingenious', 'in', 'concept', 'design', 'and', 'execution', 'that', 'you', 'could', 'watch', 'it', 'on', 'a', 'postage', 'stamp', 'sized', 'screen', 'and', 'still', 'be', 'engulfed', 'by', 'its', 'charm'], ['the', 'years', 'most', 'inventive', 'comedy'], ['a', 'winning', 'animated', 'feature', 'that', 'has', 'something', 'for', 'everyone', 'on', 'the', 'age', 'spectrum']]\n",
      "number of tokens: 280092\n",
      "number of unique tokens: 22424\n",
      "number of documents: 14072\n"
     ]
    }
   ],
   "source": [
    "# This is a bit of text cleanup\n",
    "word_bag_list = []\n",
    "for doc in documents:\n",
    "    cleaned = doc.lower().replace('-', ' ')  # make lowercase and split hyphenated words in two\n",
    "    for c in string.punctuation:  # strip punctuation marks.\n",
    "        cleaned = cleaned.replace(c, '')\n",
    "    word_bag_list.append(wordpunct_tokenize(cleaned))\n",
    "\n",
    "# How do things look?\n",
    "print 'a few tokens:', word_bag_list[:3]\n",
    "\n",
    "# this flattens the nested lists into one big list for some stats\n",
    "token_list = []\n",
    "for tokens in word_bag_list:\n",
    "    token_list.extend(tokens)\n",
    "print 'number of tokens:', len(token_list)\n",
    "print 'number of unique tokens:', len(set(token_list))\n",
    "print 'number of documents:', len(word_bag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 0.6140562819783968),\n",
       " ('a', 0.5035531552018192),\n",
       " ('and', 0.48969584991472426),\n",
       " ('of', 0.4640420693575895),\n",
       " ('is', 0.3320068220579875),\n",
       " ('to', 0.32106310403638433),\n",
       " ('in', 0.23848777714610575),\n",
       " ('that', 0.20082433200682206),\n",
       " ('its', 0.1991898806139852),\n",
       " ('it', 0.1960631040363843),\n",
       " ('with', 0.15513075611142696),\n",
       " ('but', 0.15157760090960773),\n",
       " ('this', 0.1467453098351336),\n",
       " ('movie', 0.12933484934621944),\n",
       " ('film', 0.12926378624218307),\n",
       " ('for', 0.1286242183058556),\n",
       " ('as', 0.12784252416145536),\n",
       " ('an', 0.10993462194428652),\n",
       " ('be', 0.08484934621944286),\n",
       " ('on', 0.08449403069926094)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the document frequency of all the unique tokens in the bags of words.\n",
    "\n",
    "df = Counter()  # initialize this dict-like thing.\n",
    "\n",
    "for doc in word_bag_list:\n",
    "    # FILL IN CODE\n",
    "    # count up the times words appear in INDIVIDUAL documents (not the total across all documents)\n",
    "    for token in set(doc):\n",
    "        df[token] += 1\n",
    "\n",
    "# normalize the counts by the number of documents (are you getting zeros? Think datatypes.)\n",
    "for token in df:\n",
    "    df[token] = df[token] / float(len(documents))\n",
    "\n",
    "# this prints the 20 highest-scoring words and their scores\n",
    "df.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words\n",
    "\n",
    "Which words are likely to be stop words? The ones that show up in the most documents! These terms with the largest document frequency are the stopwords! The threshold above which you call something a stopword is up to you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf\n",
    "\n",
    "More interesting than stop-words is the tf-idf score. This tells us which words are most discriminative between documents. Words that occur a lot in one document but doesn't occur in many documents will tell you something special about the document:\n",
    "\n",
    "$$\n",
    "\\text{tf-idf} = tf \\cdot -\\log{df}\n",
    "$$\n",
    "\n",
    "recall that:\n",
    "\n",
    "$$\n",
    "\\log{x} = -\\log{1 \\over x}\n",
    "$$\n",
    "\n",
    "What are the most discriminative words in the first few documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('engulfed', 0.38207769145669573), ('postage', 0.35435180423429802), ('sized', 0.32662591701190019), ('stamp', 0.30424128549448326), ('ingenious', 0.26874915769444713)]\n",
      "[('inventive', 1.1776761280575496), ('years', 0.8588893828779226), ('comedy', 0.65543605303509112), ('most', 0.59453821488145864), ('the', 0.097533738115198984)]\n",
      "[('spectrum', 0.65025615367302192), ('winning', 0.47574203511007074), ('everyone', 0.43231666566869759), ('age', 0.39485397527852278), ('animated', 0.39393272981339084)]\n",
      "[('equal', 0.39185708991301349), ('sports', 0.37253332126759958), ('provocative', 0.34790330157005933), ('technical', 0.34201603930200153), ('achievement', 0.34201603930200153)]\n",
      "[('catalog', 0.63679615242782617), ('hyperrealist', 0.63679615242782617), ('1995', 0.49031451393874498), ('toy', 0.45195690427850749), ('generated', 0.41464918508281268)]\n",
      "[('ushered', 0.23879855716043485), ('revived', 0.23879855716043485), ('lion', 0.19400457042973349), ('repetition', 0.19400457042973349), ('landmark', 0.19015080343405202)]\n",
      "[('conceptual', 0.35377564023768121), ('wholl', 0.31308629621293643), ('verbal', 0.28741417841441996), ('defined', 0.27239695218819165), ('appreciated', 0.27239695218819165)]\n",
      "[('anthropomorphism', 0.36738239563143821), ('toys', 0.28287375804158366), ('will', 0.28198755807440296), ('marvel', 0.2607443678529851), ('irresistible', 0.25413474258657515)]\n",
      "[('foray', 0.30812717052959332), ('invested', 0.26340799759024208), ('cleverness', 0.24104841112056638), ('imagery', 0.20698219582220143), ('generated', 0.20063670245942547)]\n",
      "[('toys', 0.3343053504127807), ('speak', 0.31422204349100619), ('tim', 0.2957918113042714), ('hanks', 0.28972220254861131), ('guys', 0.2827153534655541)]\n",
      "[('coaster', 0.63518117535962337), ('roller', 0.63518117535962337), ('visionary', 0.61630486947069207), ('ride', 0.51455654439152432), ('result', 0.47831662229718214)]\n",
      "[('wondrously', 0.38884037739511923), ('holiday', 0.31464301463099781), ('generated', 0.29617798934486617), ('imaginative', 0.28981935160084127), ('town', 0.28981935160084127)]\n",
      "[('popper', 0.70444416647910701), ('d', 0.53470400587402034), ('3', 0.50718386530147219), ('disneys', 0.49508536448109741), ('eye', 0.4635798533210933)]\n",
      "[('docter', 0.30812717052959332), ('pete', 0.30812717052959332), ('ranft', 0.30812717052959332), ('throats', 0.2857675840599177), ('lasseter', 0.2857675840599177)]\n",
      "[('overcommercialization', 0.95519422864173942), ('negative', 0.84533299977492848), ('involves', 0.73547177090811755), ('toy', 0.67793535641776126), ('disneys', 0.59410243737731694)]\n",
      "[('technically', 1.1521474928003559), ('toy', 1.1298922606962687), ('flawless', 1.0768166388431797), ('nearly', 0.87921269456688977), ('story', 0.53005507319758949)]\n",
      "[('playthings', 0.34114079594347829), ('eager', 0.27714938632819064), ('andy', 0.26266848961004197), ('draw', 0.26266848961004197), ('20th', 0.25550167905782223)]\n",
      "[('miraculous', 0.24862735287497895), ('i', 0.20943986701820069), ('toy', 0.19939275188757682), ('imagine', 0.17900607010640196), ('produced', 0.17637041838023471)]\n",
      "[('changer', 0.63679615242782617), ('puns', 0.56355533318328566), ('added', 0.45195690427850749), ('voice', 0.4123097637620613), ('game', 0.35549563874941914)]\n",
      "[('gloomy', 0.61096187492179188), ('generating', 0.57480774959519676), ('grotesque', 0.56574751608316731), ('extravaganza', 0.56574751608316731), ('despair', 0.53746099453506591)]\n",
      "[('extravaganza', 0.91933971363514688), ('calculated', 0.86411061960026692), ('special', 0.56683280620030874), ('effects', 0.53745860729634554), ('entertaining', 0.51587403361699891)]\n",
      "[('matthau', 0.68144577737364997), ('walter', 0.5850793951816986), ('lemmon', 0.57480774959519676), ('awfully', 0.53176038129247194), ('jack', 0.48715895858070718)]\n",
      "[('regrettably', 3.8800914085946694), ('mediocre', 3.2781050064317014)]\n",
      "[('bickering', 0.46962944431940468), ('neil', 0.43112126762162989), ('simon', 0.40859542828228745), ('disappointed', 0.39261309092385521), ('expect', 0.31559673752830575)]\n",
      "[('1993', 0.26227697025386487), ('list', 0.20677911120441311), ('10', 0.20304760828578441), ('wont', 0.18936204399555087), ('among', 0.18877258077626463)]\n",
      "[('ariels', 0.31839807621391308), ('grumps', 0.31839807621391308), ('dictated', 0.29529317019524831), ('practiced', 0.29529317019524831), ('progress', 0.25353440457873599)]\n",
      "[('grumpys', 0.73476479126287642), ('poke', 0.65025615367302192), ('somewhere', 0.49029911200534226), ('cheap', 0.48715895858070718), ('worthy', 0.46816972181674366)]\n",
      "[('melrose', 0.56187895802455257), ('escapes', 0.48033223089985316), ('aura', 0.43955886733750338), ('queasy', 0.43263045347536322), ('problems', 0.36586692801424647)]\n",
      "[('rigorous', 0.34023533022072933), ('examination', 0.30644657121171559), ('undemanding', 0.30644657121171559), ('diverting', 0.27756543868838457), ('sit', 0.26067105918387767)]\n",
      "[('flaring', 0.39799759526739137), ('rejoice', 0.39799759526739137), ('lollygags', 0.39799759526739137), ('stomp', 0.39799759526739137), ('sputtering', 0.34023533022072933)]\n",
      "[('mcmillans', 0.73476479126287642), ('celebratory', 0.73476479126287642), ('sister', 0.62812676348442342), ('terry', 0.61096187492179188), ('selling', 0.51682530325855214)]\n",
      "[('crippling', 0.26844833654113487), ('bashing', 0.26844833654113487), ('exhale', 0.25616151508331164), ('crack', 0.24068195072676649), ('psyche', 0.2351570550663436)]\n",
      "[('mishandles', 0.73476479126287642), ('whitaker', 0.65025615367302192), ('clumsily', 0.59693713978379537), ('tour', 0.52148873570597021), ('girl', 0.44910540151639128)]\n",
      "[('grappling', 0.35377564023768121), ('exhale', 0.31308629621293643), ('waiting', 0.24101554995162852), ('issues', 0.22906097986781182), ('uneven', 0.21362046861107897)]\n",
      "[('bernadine', 0.39799759526739137), ('populating', 0.39799759526739137), ('exception', 0.30205654972597284), ('capture', 0.29445981819289141), ('sympathy', 0.27756543868838457)]\n",
      "[('men', 0.36008337559524572), ('musing', 0.3293773202212894), ('wardrobe', 0.30547569330542929), ('her', 0.28836581091653324), ('i', 0.24555018891789049)]\n",
      "[('fraker', 0.29849819645054354), ('equilibriums', 0.29849819645054354), ('rattled', 0.26416656242966513), ('retro', 0.23768850429256502), ('fluff', 0.23768850429256502)]\n",
      "[('crook', 0.23879855716043485), ('somebodys', 0.23879855716043485), ('jobs', 0.22146987764643625), ('bruised', 0.22146987764643625), ('stare', 0.20414119813243761)]\n",
      "[('warring', 0.28945279655810285), ('taciturn', 0.28945279655810285), ('braying', 0.28945279655810285), ('ashley', 0.25616151508331164), ('val', 0.2474438765241668)]\n",
      "[('robberies', 0.36738239563143821), ('tautly', 0.34072288868682499), ('elaborately', 0.2925396975908493), ('satisfaction', 0.28287375804158366), ('edited', 0.26873049726753295)]\n",
      "[('reinvests', 0.27291263675478267), ('postmodernist', 0.24152371422140811), ('modernist', 0.24152371422140811), ('artifice', 0.23330422643707155), ('lay', 0.21731520392463088)]\n",
      "[('proportion', 0.25136690227414193), ('nowadays', 0.23312618699624865), ('lengthy', 0.21488547171835537), ('robbers', 0.20901327299956035), ('1995', 0.19354520287055724)]\n",
      "[('asphalt', 0.34114079594347829), ('interpersonal', 0.34114079594347829), ('discussions', 0.31638553949490889), ('fitted', 0.31638553949490889), ('robbers', 0.28366087049940331)]\n",
      "[('invited', 0.35222208323955351), ('fortunately', 0.31135419769739825), ('mann', 0.29445981819289141), ('exceptional', 0.28803687320008897), ('party', 0.27317541720264177)]\n",
      "[('forges', 0.63277107898981777), ('180', 0.58326056609267884), ('heat', 0.46481570347814072), ('mark', 0.44956041059970803), ('consistently', 0.4347290274012619)]\n",
      "[('occupies', 0.76848454524993492), ('exalted', 0.74233162957250043), ('position', 0.72204585218029949), ('countless', 0.70547116519903086), ('heat', 0.5915836226085428)]\n",
      "[('niros', 0.59693713978379537), ('devoted', 0.56574751608316731), ('parody', 0.49357292849909573), ('robert', 0.41981520955348417), ('himself', 0.40163145739469719)]\n",
      "[('indeed', 0.32082240352441227), ('decade', 0.32082240352441227), ('finest', 0.29071363340670131), ('crime', 0.27814791199265598), ('period', 0.27287988620976472)]\n",
      "[('beside', 0.35502817066510889), ('expose', 0.33739925292127559), ('weaknesses', 0.33069704945052525), ('pale', 0.32489133672771991), ('mouse', 0.32489133672771991)]\n",
      "[('superlative', 0.24820326168697793), ('rife', 0.23351564827304869), ('meticulously', 0.22983492840878672), ('controlled', 0.22654241229447963), ('mann', 0.22084486364466857)]\n",
      "[('squealing', 0.49215528365874717), ('tires', 0.49215528365874717), ('banalities', 0.45364710696097243), ('drowning', 0.43112126762162989), ('bodies', 0.41513893026319765)]\n",
      "[('elevate', 0.60380928555352031), ('manns', 0.60380928555352031), ('writing', 0.41198233232136661), ('michael', 0.39224994827649817), ('material', 0.33405321165830298)]\n",
      "[('colossal', 2.8177766659164281), ('disappointment', 2.1388160234960814), ('a', 0.2286886669973256)]\n",
      "[('generates', 0.72493571934233492), ('heat', 0.65074198486939716), ('lots', 0.62197377762421902), ('energy', 0.50860341677628107), ('light', 0.50746054719391875)]\n",
      "[('major', 0.61406562174247159), ('touts', 0.26533173017826089), ('boosters', 0.26533173017826089), ('yawns', 0.24607764182937358), ('basics', 0.24607764182937358)]\n",
      "[('watertight', 0.50273380454828387), ('pacino', 0.35361731275585145), ('niro', 0.35060897518532785), ('each', 0.28302921139588194), ('acted', 0.27276286494475643)]\n",
      "[('bulk', 0.50273380454828387), ('well', 0.37167567758609132), ('cop', 0.34249578151020899), ('crime', 0.29278727578174313), ('thought', 0.27838223207242518)]\n",
      "[('glittering', 0.38884037739511923), ('measure', 0.3228263601989339), ('somehow', 0.27687012705400121), ('lead', 0.27574962717732526), ('formula', 0.27253813761563328)]\n",
      "[('dropped', 0.30812717052959332), ('glibly', 0.30812717052959332), ('quipping', 0.30812717052959332), ('tugs', 0.30812717052959332), ('trouping', 0.2857675840599177)]\n",
      "[('tastelessness', 1.1073493882321812), ('wilders', 0.89425587670237794), ('artistic', 0.78201317755163313), ('strength', 0.76884311309440478), ('major', 0.69082382446028057)]\n",
      "[('callousness', 0.31839807621391308), ('world', 0.28192882048526569), ('barriers', 0.28177766659164283), ('confrontation', 0.2490833581579186), ('breaking', 0.23556785455431314)]\n",
      "[('hampered', 0.42977094343671074), ('unimaginative', 0.38709040574111447), ('delightfully', 0.37652879019047492), ('cinderella', 0.36773646994504511), ('variation', 0.36773646994504511)]\n",
      "[('as', 0.4376502239291194), ('wilderhas', 0.20323281460462539), ('column', 0.20323281460462539), ('screed', 0.18848500225228615), ('yesterdays', 0.1798580850584954)]\n",
      "[('lump', 2.7218826417658346), ('joyless', 2.4515725696937247), ('a', 0.2286886669973256)]\n",
      "[('reuniting', 0.41530183853988667), ('abilities', 0.36753608685866451), ('timecop', 0.35502817066510889), ('biter', 0.34532627712970843), ('exploits', 0.32489133672771991)]\n",
      "[('hand', 0.71240408528834287), ('mounts', 0.52110559446220295), ('steadily', 0.49725470574995789), ('pyrotechnics', 0.41099958405622689), ('combat', 0.39521934955065752)]\n",
      "[('claude', 0.29416682866604787), ('sudden', 0.27239695218819165), ('1995', 0.27239695218819165), ('building', 0.26849471086753141), ('damme', 0.24884181268004363)]\n",
      "[('go', 0.36949570616671989), ('puck', 0.26533173017826089), ('goalie', 0.26533173017826089), ('booby', 0.26533173017826089), ('jollies', 0.26533173017826089)]\n",
      "[('dropping', 0.31770017495933173), ('claude', 0.31770017495933173), ('jaw', 0.30424128549448326), ('damme', 0.26874915769444713), ('loaded', 0.26224840051453613)]\n",
      "[('manipulation', 0.28170489397637333), ('sudden', 0.27239695218819165), ('common', 0.23929258640959547), ('create', 0.22209608240473999), ('death', 0.20963414771506544)]\n",
      "[('patently', 0.34114079594347829), ('atop', 0.34114079594347829), ('roof', 0.31638553949490889), ('overtime', 0.30190464277676016), ('arena', 0.29163028304633942)]\n",
      "[('thwart', 0.3293773202212894), ('chores', 0.3293773202212894), ('defuse', 0.3293773202212894), ('boothe', 0.3293773202212894), ('bombs', 0.3293773202212894)]\n",
      "[('connecting', 0.27268806444352528), ('worthless', 0.26340799759024208), ('choreographed', 0.24535587539877679), ('hyams', 0.23077571011674269), ('flair', 0.21673319168907026)]\n",
      "[('brosnan', 0.31839807621391308), ('handicap', 0.29529317019524831), ('recapture', 0.27218826417658348), ('humorless', 0.2490833581579186), ('amused', 0.2490833581579186)]\n",
      "[('onatopp', 0.3293773202212894), ('xenia', 0.3293773202212894), ('domination', 0.30547569330542929), ('fighting', 0.2738794611718377), ('returns', 0.24997783425597753)]\n",
      "[('clinging', 0.31839807621391308), ('mite', 0.29529317019524831), ('entries', 0.27218826417658348), ('cycle', 0.26475014579944312), ('deny', 0.25867276057297794)]\n",
      "[('kiel', 1.9103884572834788), ('missed', 1.332314105704246), ('richard', 1.199318844985596), ('you', 0.56538172885011029), ('are', 0.53492924298399314)]\n",
      "[('33', 0.23297420210774131), ('breathes', 0.19916214451945133), ('17', 0.18927275163876436), ('007', 0.18225611572530631), ('bonds', 0.17938335875807745)]\n",
      "[('bowling', 0.4662523739924973), ('recapture', 0.42977094343671074), ('campbell', 0.4180265459991207), ('matters', 0.37652879019047492), ('earlier', 0.31561022236463054)]\n",
      "[('blahs', 3.1839807621391309), ('suffers', 2.1388160234960814), ('the', 0.16255623019199827)]\n",
      "[('thundering', 1.2655421579796355), ('supercharged', 1.2076185711070406), ('numbing', 0.98755499382887646), ('brain', 0.90472378022131328), ('spectacular', 0.83405288853044079)]\n",
      "[('souviens', 0.41530183853988667), ('je', 0.41530183853988667), ('plate', 0.36753608685866451), ('vanity', 0.31977033517744241), ('read', 0.27897600306470627)]\n",
      "[('facsimile', 0.43417919483715423), ('weariness', 0.40267250481170225), ('unfold', 0.33965912476079813), ('reasonable', 0.32951623606469765), ('certain', 0.24443431893280715)]\n",
      "[('goldeneye', 0.88587951058574499), ('babes', 0.88587951058574499), ('bs', 0.88587951058574499), ('excels', 0.84533299977492848), ('stunts', 0.60554347249509144)]\n",
      "[('proficiency', 1.0613269207130436), ('dollar', 0.90729421392194487), ('wit', 0.55890596770759482), ('top', 0.55534059942409475), ('directed', 0.5171224984962759)]\n",
      "[('spy', 0.51682530325855214), ('loved', 0.50826948517315029), ('entry', 0.50057075759184388), ('perhaps', 0.40062591265874697), ('me', 0.38693490072064263)]\n",
      "[('belated', 0.29529317019524831), ('satisfaction', 0.24515725696937249), ('bonds', 0.24515725696937249), ('observed', 0.24515725696937249), ('i', 0.23736518262062747)]\n",
      "[('sweats', 0.59699639290108708), ('part', 0.58846939999485037), ('exudes', 0.52833312485933026), ('capra', 0.43205530980013346), ('corny', 0.42774325533219898)]\n",
      "[('oval', 0.25816060233560523), ('sorkin', 0.25816060233560523), ('zinger', 0.23942689475290405), ('dandy', 0.23942689475290405), ('twinkling', 0.23942689475290405)]\n",
      "[('notably', 1.0095765195184849), ('genial', 0.98755499382887646), ('inspired', 0.82724888153197584), ('entertaining', 0.58957032413371302), ('if', 0.44765390721159748)]\n",
      "[('coasts', 0.63679615242782617), ('middlebrow', 0.52950029159888623), ('genial', 0.46085899712014233), ('fare', 0.40786367212881652), ('stars', 0.34896361152540561)]\n",
      "[('bustling', 0.76848454524993492), ('impassioned', 0.70547116519903086), ('sturges', 0.64245778514812679), ('capra', 0.62844408698201237), ('as', 0.37399200953942929)]\n",
      "[('unwatchable', 0.48033223089985316), ('reiner', 0.41099958405622689), ('president', 0.41099958405622689), ('rob', 0.39521934955065752), ('nevertheless', 0.37743812179342612)]\n",
      "[('swept', 0.53375005319553981), ('dislike', 0.53375005319553981), ('you', 0.40384409203579308), ('wont', 0.39224994827649817), ('either', 0.38411107260869692)]\n",
      "[('make', 0.54295456484540749), ('you', 0.42403629663758274), ('vote', 0.38030160686810405), ('president', 0.34934964644779287), ('change', 0.31665332307745969)]\n",
      "[('bias', 0.40267250481170225), ('abundance', 0.35273558259951543), ('smarts', 0.34572873351645822), ('spare', 0.3343053504127807), ('president', 0.31759058767981169)]\n",
      "[('entertainment', 2.1794927177635919), ('great', 1.9307914160466668)]\n",
      "[('modulated', 2.0414119813243761), ('charmer', 2.0414119813243761), ('well', 0.88272973426696688), ('a', 0.1715165002479942)]\n",
      "[('simpering', 0.56187895802455257), ('darn', 0.43955886733750338), ('president', 0.41099958405622689), ('20', 0.37493461506290876), ('affecting', 0.36800620120076855)]\n",
      "[('frothy', 7.9425043739832937)]\n"
     ]
    }
   ],
   "source": [
    "# calculate the term frequency of all the unique tokens in all of the bags of words.\n",
    "\n",
    "for doc in word_bag_list[:100]:\n",
    "    tf = Counter()  # initialize this dict-like thing.\n",
    "    tfidf = Counter()\n",
    "    \n",
    "    # FILL IN CODE\n",
    "\n",
    "    # calculate term frequencies\n",
    "    for token in doc:\n",
    "        tf[token] += 1\n",
    "    total = float(sum(tf.values()))\n",
    "\n",
    "    # calculate tf-idf scores\n",
    "    for token in tf:\n",
    "        tfidf[token] = (tf[token] / total) * (-np.log(df[token]))\n",
    "\n",
    "    # this prints most significant words in the document\n",
    "    print tfidf.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sci-Kit Learn\n",
    "\n",
    "Scikit-Learn comes with utilities to do these calculations for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.33171187  0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "tfidf_vec = TfidfVectorizer(stop_words='english')\n",
    "output = tfidf_vec.fit_transform(documents)\n",
    "print output.toarray()[20:30, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset(['all', 'six', 'less', 'being', 'indeed', 'over', 'move', 'anyway', 'four', 'not', 'own', 'through', 'yourselves', 'fify', 'where', 'mill', 'only', 'find', 'before', 'one', 'whose', 'system', 'how', 'somewhere', 'with', 'thick', 'show', 'had', 'enough', 'should', 'to', 'must', 'whom', 'seeming', 'under', 'ours', 'has', 'might', 'thereafter', 'latterly', 'do', 'them', 'his', 'around', 'than', 'get', 'very', 'de', 'none', 'cannot', 'every', 'whether', 'they', 'front', 'during', 'thus', 'now', 'him', 'nor', 'name', 'several', 'hereafter', 'always', 'who', 'cry', 'whither', 'this', 'someone', 'either', 'each', 'become', 'thereupon', 'sometime', 'side', 'two', 'therein', 'twelve', 'because', 'often', 'ten', 'our', 'eg', 'some', 'back', 'up', 'go', 'namely', 'towards', 'are', 'further', 'beyond', 'ourselves', 'yet', 'out', 'even', 'will', 'what', 'still', 'for', 'bottom', 'mine', 'since', 'please', 'forty', 'per', 'its', 'everything', 'behind', 'un', 'above', 'between', 'it', 'neither', 'seemed', 'ever', 'across', 'she', 'somehow', 'be', 'we', 'full', 'never', 'sixty', 'however', 'here', 'otherwise', 'were', 'whereupon', 'nowhere', 'although', 'found', 'alone', 're', 'along', 'fifteen', 'by', 'both', 'about', 'last', 'would', 'anything', 'via', 'many', 'could', 'thence', 'put', 'against', 'keep', 'etc', 'amount', 'became', 'ltd', 'hence', 'onto', 'or', 'con', 'among', 'already', 'co', 'afterwards', 'formerly', 'within', 'seems', 'into', 'others', 'while', 'whatever', 'except', 'down', 'hers', 'everyone', 'done', 'least', 'another', 'whoever', 'moreover', 'couldnt', 'throughout', 'anyhow', 'yourself', 'three', 'from', 'her', 'few', 'together', 'top', 'there', 'due', 'been', 'next', 'anyone', 'eleven', 'much', 'call', 'therefore', 'interest', 'then', 'thru', 'themselves', 'hundred', 'was', 'sincere', 'empty', 'more', 'himself', 'elsewhere', 'mostly', 'on', 'fire', 'am', 'becoming', 'hereby', 'amongst', 'else', 'part', 'everywhere', 'too', 'herself', 'former', 'those', 'he', 'me', 'myself', 'made', 'twenty', 'these', 'bill', 'cant', 'us', 'until', 'besides', 'nevertheless', 'below', 'anywhere', 'nine', 'can', 'of', 'your', 'toward', 'my', 'something', 'and', 'whereafter', 'whenever', 'give', 'almost', 'wherever', 'is', 'describe', 'beforehand', 'herein', 'an', 'as', 'itself', 'at', 'have', 'in', 'seem', 'whence', 'ie', 'any', 'fill', 'again', 'hasnt', 'inc', 'thereby', 'thin', 'no', 'perhaps', 'latter', 'meanwhile', 'when', 'detail', 'same', 'wherein', 'beside', 'also', 'that', 'other', 'take', 'which', 'becomes', 'you', 'if', 'nobody', 'see', 'though', 'may', 'after', 'upon', 'most', 'hereupon', 'eight', 'but', 'serious', 'nothing', 'such', 'why', 'a', 'off', 'whereby', 'third', 'i', 'whole', 'noone', 'sometimes', 'well', 'amoungst', 'yours', 'their', 'rather', 'without', 'so', 'five', 'the', 'first', 'whereas', 'once'])\n"
     ]
    }
   ],
   "source": [
    "print tfidf_vec.get_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
