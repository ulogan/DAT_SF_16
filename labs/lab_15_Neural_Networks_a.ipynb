{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GA Data Science 16 (DAT16) - Lab 15\n",
    "\n",
    "### Neural Networks\n",
    "\n",
    "Francesco Mosconi, Justin Breucop\n",
    "\n",
    "### Today\n",
    "\n",
    "Implementing a Neural Network from Scratch\n",
    "\n",
    "This lab is inspired by [this](http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/) blog post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "\n",
    "# Display plots inline and change default figure size\n",
    "from bokeh.plotting import figure,gridplot,show,output_notebook\n",
    "from bokeh.models import Range1d\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a dataset\n",
    "\n",
    "Let's start by generating a dataset we can play with. Fortunately, [scikit-learn](http://scikit-learn.org/) has some useful dataset generators, so we don't need to write the code ourselves. We will go with the [`make_moons`](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate a dataset and plot it\n",
    "np.random.seed(0)\n",
    "X, y = sklearn.datasets.make_moons(200, noise=0.20)\n",
    "\n",
    "p = figure(tools='', title=\"Two half moon distributed datasets\")\n",
    "color_dict = {0:'blue',1:'red'}\n",
    "y_colors = [color_dict[label] for label in y]\n",
    "\n",
    "# These are glyphs\n",
    "p.circle(X[:,0], X[:,1],size=10, color=y_colors, alpha = 0.5)\n",
    "\n",
    "# show the results\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we generated has two classes, plotted as red and blue points. You can think of the blue dots as male patients and the red dots as female patients, with the x- and y- axis being medical measurements. \n",
    "\n",
    "Our goal is to train a Machine Learning classifier that predicts the correct class (male of female) given the x- and y- coordinates. Note that the data is not *linearly separable*, we can't draw a straight line that separates the two classes. This means that linear classifiers, such as Logistic Regression, won't be able to fit the data unless you hand-engineer non-linear features (such as polynomials) that work well for the given dataset.\n",
    "\n",
    "In fact, that's one of the major advantages of Neural Networks. You don't need to worry about [feature engineering](http://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/). The hidden layer of a neural network will learn features for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "To demonstrate the point let's train a Logistic Regression classifier. It's input will be the x- and y-values and the output the predicted class (0 or 1). To make our life easy we use the Logistic Regression class from `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train the logistic rgeression classifier\n",
    "clf = sklearn.linear_model.LogisticRegressionCV()\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Helper function to plot a decision boundary.\n",
    "# If you don't fully understand this function don't worry, it just generates the contour plot below.\n",
    "def plot_decision_boundary(pred_func, point_size = 10):\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "\n",
    "    # Generate a grid of points with distance h between them\n",
    "    h = 0.01\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # Predict the function value for the whole gid\n",
    "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    control_row = np.ones(Z.shape[1])*Z.max()+0.001\n",
    "    #NOTE: control row adds a max value outside of the grid to correct colors. \n",
    "    # Don't worry about explaining control_row\n",
    "    Z = np.vstack((Z,control_row))\n",
    "\n",
    "\n",
    "    p1 = figure(tools='',\n",
    "                x_range=(xx.min(), xx.max()), \n",
    "                y_range=(yy.min(), yy.max()))\n",
    "\n",
    "    # Plot the contour and training examples\n",
    "    p1.image(image=[Z], x=[xx.min()], y=[yy.min()],\n",
    "             dw=[xx.max()-xx.min()], dh=[yy.max()-yy.min()],\n",
    "             palette='RdYlBu11')\n",
    "\n",
    "    p1.circle(X[:, 0], X[:, 1], \n",
    "              color=y_colors,\n",
    "              line_color='black',\n",
    "              size=point_size,alpha = 0.7)\n",
    "    return(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the decision boundary\n",
    "p1 = plot_decision_boundary(lambda x: clf.predict(x))\n",
    "p1.title = \"Logistic Regression\"\n",
    "show(p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph shows the decision boundary learned by our Logistic Regression classifier. It separates the data as good as it can using a straight line, but it's unable to capture the \"moon shape\" of our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of the Neural Network\n",
    "\n",
    "Now we are ready for our implementation. We start by defining some useful variables and parameters for gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_examples = len(X) # training set size\n",
    "nn_input_dim = 2 # input layer dimensionality\n",
    "nn_output_dim = 2 # output layer dimensionality\n",
    "\n",
    "# Gradient descent parameters (I picked these by hand)\n",
    "epsilon = 0.01 # learning rate for gradient descent\n",
    "reg_lambda = 0.01 # regularization strength "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's implement the loss function we defined above. We use this to evaluate how well our model is doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise: add comments to each line of the next function\n",
    "def calculate_loss(model):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    z1 = X.dot(W1) + b1\n",
    "    a1 = np.tanh(z1)\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    exp_scores = np.exp(z2)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    corect_logprobs = -np.log(probs[range(num_examples), y])\n",
    "    data_loss = np.sum(corect_logprobs)\n",
    "    data_loss += reg_lambda/2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)))\n",
    "    return 1./num_examples * data_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also implement a helper function to calculate the output of the network. It does forward propagation and returns the class with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise: add comments to each line of the next function\n",
    "def predict(model, x):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    z1 = x.dot(W1) + b1\n",
    "    a1 = np.tanh(z1)\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    exp_scores = np.exp(z2)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    return np.argmax(probs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, here comes the function to train our Neural Network. It implements batch gradient descent using the backpropagation derivates we found above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Exercise: comments the blocks of code with high level description\n",
    "def build_model(nn_hdim, num_passes=20000, print_loss=False):\n",
    "    # block ...\n",
    "    np.random.seed(0)\n",
    "    W1 = np.random.randn(nn_input_dim, nn_hdim) / np.sqrt(nn_input_dim)\n",
    "    b1 = np.zeros((1, nn_hdim))\n",
    "    W2 = np.random.randn(nn_hdim, nn_output_dim) / np.sqrt(nn_hdim)\n",
    "    b2 = np.zeros((1, nn_output_dim))\n",
    "\n",
    "    # block ...\n",
    "    model = {}\n",
    "    \n",
    "    # block ...\n",
    "    for i in xrange(0, num_passes):\n",
    "\n",
    "        # block ...\n",
    "        z1 = X.dot(W1) + b1\n",
    "        a1 = np.tanh(z1)\n",
    "        z2 = a1.dot(W2) + b2\n",
    "        exp_scores = np.exp(z2)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "        # block ...\n",
    "        delta3 = probs\n",
    "        delta3[range(num_examples), y] -= 1\n",
    "        dW2 = (a1.T).dot(delta3)\n",
    "        db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "        delta2 = delta3.dot(W2.T) * (1 - np.power(a1, 2))\n",
    "        dW1 = np.dot(X.T, delta2)\n",
    "        db1 = np.sum(delta2, axis=0)\n",
    "\n",
    "        # block ...\n",
    "        dW2 += reg_lambda * W2\n",
    "        dW1 += reg_lambda * W1\n",
    "\n",
    "        # block ...\n",
    "        W1 += -epsilon * dW1\n",
    "        b1 += -epsilon * db1\n",
    "        W2 += -epsilon * dW2\n",
    "        b2 += -epsilon * db2\n",
    "        \n",
    "        # block ...\n",
    "        model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "        \n",
    "        # block ...\n",
    "        if print_loss and i % 1000 == 0:\n",
    "            print \"Loss after iteration %i: %f\" %(i, calculate_loss(model))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A network with a hidden layer of size 3\n",
    "\n",
    "Let's see what happens if we train a network with a hidden layer size of 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build a model with a 3-dimensional hidden layer\n",
    "model = build_model(3, print_loss=True)\n",
    "\n",
    "# Plot the decision boundary\n",
    "p2 = plot_decision_boundary(lambda x: predict(model, x))\n",
    "p2.title = \"Decision Boundary for hidden layer size 3\"\n",
    "show(p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! This looks pretty good. Our neural networks was able to find a decision boundary that successfully separates the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Varying the hidden layer size\n",
    "\n",
    "In the example above we picked a hidden layer size of 3. Let's now get a sense of how varying the hidden layer size affects the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden_layer_dimensions = [1, 2, 3, 4, 5, 20, 30, 50]\n",
    "\n",
    "row = []\n",
    "\n",
    "for i, nn_hdim in enumerate(hidden_layer_dimensions):\n",
    "    model = build_model(nn_hdim)\n",
    "    p = plot_decision_boundary(lambda x: predict(model, x), point_size=8)\n",
    "    p.title = 'Hidden Layer size %d' % nn_hdim\n",
    "    p.plot_width=360\n",
    "    p.plot_height=360\n",
    "    row.append(p)\n",
    "\n",
    "grid = np.array(row).reshape(len(row)/2, 2)\n",
    "show(gridplot(grid.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that while a hidden layer of low dimensionality nicely capture the general trend of our data, but higher dimensionalities are prone to overfitting. They are \"memorizing\" the data as opposed to fitting the general shape. If we were to evaluate our model on a separate test set (and you should!) the model with a smaller hidden layer size would likely perform better because it generalizes better. We could counteract overfitting with stronger regularization, but picking the a correct size for hidden layer is a much more \"economical\" solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Encapsulate the gradient calculation in a function:\n",
    "\n",
    "    def evaluate_gradient(X, y, l, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "    l : len(y) is the number of data points\n",
    "    \"\"\"\n",
    "\n",
    "modify the build_model function to take the newly defined function as parameter\n",
    "\n",
    "    def build_model_g_func(nn_hdim, num_passes=20000, print_loss=False, gradient_func=evaluate_gradient):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_gradient(X, y, l, W1, b1, W2, b2):\n",
    "    # your code here....\n",
    "    \n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "def build_model_g_func(nn_hdim, num_passes=20000, print_loss=False, gradient_func=evaluate_gradient):\n",
    "    # your code here....\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that you still get the same results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build a model with a 3-dimensional hidden layer\n",
    "model = build_model_g_func(3, print_loss=True)\n",
    "\n",
    "# Plot the decision boundary\n",
    "p2 = plot_decision_boundary(lambda x: predict(model, x))\n",
    "p2.title = \"Decision Boundary for hidden layer size 3\"\n",
    "show(p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 2\n",
    "\n",
    "Instead of batch gradient descent, use minibatch gradient descent ([more info](http://cs231n.github.io/optimization-1/#gd)) to train the network. Minibatch gradient descent typically performs better in practice.\n",
    "\n",
    "Define a new function:\n",
    "\n",
    "    def build_model_sample(nn_hdim, num_passes=20000, print_loss=False,\n",
    "                       gradient_func=evaluate_gradient, sample_size = 32):\n",
    "\n",
    "and test it on the same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model_sample(nn_hdim, num_passes=20000, print_loss=False,\n",
    "                       gradient_func=evaluate_gradient, sample_size = 32):\n",
    "    \n",
    "    # your code here....\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build a model with a 3-dimensional hidden layer\n",
    "model = build_model_sample(3, print_loss=True)\n",
    "\n",
    "# Plot the decision boundary\n",
    "p2 = plot_decision_boundary(lambda x: predict(model, x))\n",
    "p2.title = # set an appropriate title....\n",
    "show(p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "We used a fixed learning rate $\\epsilon$ for gradient descent. Implement an annealing schedule for the gradient descent learning rate ([more info](http://cs231n.github.io/neural-networks-3/#anneal)). \n",
    "\n",
    "Modify the build_model function to \n",
    "\n",
    "    def build_model_anneal(nn_hdim, num_passes=20000, print_loss=False,\n",
    "                            gradient_func=evaluate_gradient):\n",
    "                            \n",
    "to allow for annealing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model_anneal(nn_hdim, num_passes=20000, print_loss=False,\n",
    "                       gradient_func=evaluate_gradient):\n",
    "\n",
    "    # your code here....\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build a model with a 3-dimensional hidden layer\n",
    "model = build_model_anneal(3, print_loss=True)\n",
    "\n",
    "# Plot the decision boundary\n",
    "p2 = plot_decision_boundary(lambda x: predict(model, x))\n",
    "p2.title = # title...\n",
    "show(p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "We used a $\\tanh$ activation function for our hidden layer. Experiment with other activation functions (some are mentioned above). Note that changing the activation function also means changing the backpropagation derivative.\n",
    "\n",
    "You'll need to define the following functions:\n",
    "\n",
    "    def calculate_loss_sigmoid(model):\n",
    "\n",
    "    def evaluate_gradient_sigmoid(X, y, l, W1, b1, W2, b2):\n",
    "\n",
    "    def build_model_sigmoid(nn_hdim, num_passes=20000, print_loss=False,\n",
    "                       gradient_func=evaluate_gradient_sigmoid, sample_size = 32):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "# Helper function to evaluate the total loss on the dataset\n",
    "def calculate_loss_sigmoid(model):\n",
    "\n",
    "    # your code here ...\n",
    "    \n",
    "    return 1./num_examples * data_loss\n",
    "\n",
    "\n",
    "def evaluate_gradient_sigmoid(X, y, l, W1, b1, W2, b2):\n",
    "    \n",
    "    # your code here ...\n",
    "    \n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "def build_model_sigmoid(nn_hdim, num_passes=20000, print_loss=False,\n",
    "                       gradient_func=evaluate_gradient_sigmoid, sample_size = 32):\n",
    "\n",
    "    # your code here ...\n",
    "        \n",
    "    return model\n",
    "\n",
    "# Helper function to predict an output (0 or 1)\n",
    "def predict_sigmoid(model, x):\n",
    "    \n",
    "    # your code here ...\n",
    "    \n",
    "    return np.argmax(probs, axis=1)\n",
    "\n",
    "# Build a model with a 3-dimensional hidden layer\n",
    "model = build_model_sigmoid(3, print_loss=True)\n",
    "\n",
    "# Plot the decision boundary\n",
    "p2 = plot_decision_boundary(lambda x: predict_sigmoid(model, x))\n",
    "p2.title = # title...\n",
    "show(p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "\n",
    "Extend the network from two to three classes. You will need to generate an appropriate dataset for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate a dataset and plot it\n",
    "np.random.seed(0)\n",
    "X, y = sklearn.datasets.make_classification(300, n_classes = 3, n_features=2, n_redundant=0,\n",
    "                                            n_clusters_per_class = 1)\n",
    "\n",
    "p = figure(tools='', title=\"Two half moon distributed datasets\")\n",
    "color_dict = {0:'blue',1:'yellow', 2:'red'}\n",
    "y_colors = [color_dict[label] for label in y]\n",
    "\n",
    "# These are glyphs\n",
    "p.circle(X[:,0], X[:,1],size=10, color=y_colors, alpha = 0.5)\n",
    "\n",
    "# show the results\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the global parameters of the network and re-define the following functions to allow for 3 class dataset:\n",
    "\n",
    "    def build_model(nn_hdim, nn_input_dim=2, nn_output_dim=2, num_passes=20000, print_loss=False):\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_examples = 300\n",
    "\n",
    "def build_model(nn_hdim, nn_input_dim=2, nn_output_dim=2, num_passes=20000, print_loss=False):\n",
    "    \n",
    "    # your code here ...\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Build a model with a 3-dimensional hidden layer\n",
    "model = build_model(3, 2, 3, 300, print_loss=True)\n",
    "\n",
    "# Plot the decision boundary\n",
    "p2 = plot_decision_boundary(lambda x: predict(model, x))\n",
    "p2.title = # title ... \n",
    "show(p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "\n",
    "Extend the network to four layers. Experiment with the layer size. Adding another hidden layer means you will need to adjust both the forward propagation as well as the backpropagation code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Helper function to evaluate the total loss on the dataset\n",
    "def calculate_loss(model):\n",
    "    \n",
    "    # your code here ...\n",
    "\n",
    "    return 1./num_examples * data_loss\n",
    "\n",
    "# Helper function to predict an output (0 or 1)\n",
    "def predict(model, x):\n",
    "    \n",
    "    # your code here ...\n",
    "    \n",
    "    return np.argmax(probs, axis=1)\n",
    "\n",
    "def build_model(nn_hdim1, nn_hdim2, nn_input_dim, nn_output_dim, num_examples, num_passes=20000, print_loss=False):\n",
    "    \n",
    "    # your code here ...\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Build a model with a 3-dimensional hidden layer\n",
    "model = build_model(4, 3, 2, 3, 300, print_loss=True)\n",
    "\n",
    "\n",
    "# Plot the decision boundary\n",
    "p2 = plot_decision_boundary(lambda x: predict(model, x))\n",
    "p2.title = # title ...\n",
    "show(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
