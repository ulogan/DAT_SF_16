{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "\n",
    "It's often (but not always) useful to reduce words to their roots. One reason for doing this may be that word tense or conjugation is not important for your model. It would be useful to combine variations of a word together. Then for models like Naive Bayes where each word is a feature, we can strongly reduce our feature space.\n",
    "\n",
    "Let's see what this looks like. First, let's tokenize a bit of text from the wikipedia page on data science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize  # for tokenizing our text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sample text from wikipedia\n",
    "import codecs\n",
    "text = codecs.open('data/sample.txt', \"r\", \"utf-8\").read()\n",
    "\n",
    "word_bag = wordpunct_tokenize(text)\n",
    "print 'a few tokens:', word_bag[:10]\n",
    "print 'number of tokens:', len(word_bag)\n",
    "print 'number of unique tokens:', len(set(word_bag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for common word endings to clip off. Start with the suffix, '-s', '-er', '-ing'. But be careful to only strip these tokens when they appear at the end of the word. Write rules into the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define a function to stem tokens based on rules.\n",
    "\n",
    "def stem(tokens):\n",
    "    '''rules-based stemming of a bunch of tokens'''\n",
    "    \n",
    "    new_bag = []\n",
    "    for token in tokens:\n",
    "        # define rules here\n",
    "        if token:  # edit this to test for some condition involving `token`\n",
    "            # append the stemmed token to the bag.\n",
    "        elif token:\n",
    "            # append the stemmed token to the bag.\n",
    "            # keep adding more rules in elif statements.\n",
    "        else:  # base case there no stemming is required\n",
    "            new_bag.append(token)\n",
    "\n",
    "    return new_bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check how well you're doing by running this cell:\n",
    "\n",
    "print 'initial number of unique tokens:', len(set(word_bag))\n",
    "print 'stemmed number of unique tokens:', len(set(stem(word_bag)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Do we have to refine our rules? Are we stripping away too many letters? Run this cell to see\n",
    "\n",
    "for token in stem(word_bag):\n",
    "    print token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to add more rules and see how much you can pare down the feature set, i.e. the number of unique tokens. Try not to strip too much off the words!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Porter Stemmer\n",
    "\n",
    "The classic stemmer is the Porter stemmer which is [available in NLTK](http://www.nltk.org/api/nltk.stem.html#module-nltk.stem.porter). Others are available, too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run this cell to see how the Porter Stemmer performs.\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "print 'initial number of unique tokens:', len(set(word_bag))\n",
    "print 'stemmed number of unique tokens:', len({ps.stem(token) for token in word_bag})  # this uses a set comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# examine how weird the tokens get\n",
    "\n",
    "for token in word_bag:\n",
    "    print ps.stem(token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
