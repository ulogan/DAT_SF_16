{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stop words, tf-idf\n",
    "\n",
    "Let's investigate one of the most useful feature weightings, and how stop words derive naturally from that. To start, let's load a set of small documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "try:\n",
    "    df = pd.read_csv('data/rt_critics.csv')\n",
    "except IOError:\n",
    "    print 'cannot find file'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# It seems silly to call such short blurbs 'documents', but we'll stick with the NLP nomenclature.\n",
    "\n",
    "documents = list(df['quote'])\n",
    "documents[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Frequency\n",
    "\n",
    "Let's start by calculating the document frequency for words in these documents. For this task, let's also remove all the punctuation marks and make everything lower-case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize  # for tokenizing our text\n",
    "import string  # helps with removing punctuation\n",
    "from collections import Counter  # great dict-like datastructure for counting things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a few tokens: [['so', 'ingenious', 'in', 'concept', 'design', 'and', 'execution', 'that', 'you', 'could', 'watch', 'it', 'on', 'a', 'postage', 'stamp', 'sized', 'screen', 'and', 'still', 'be', 'engulfed', 'by', 'its', 'charm'], ['the', 'years', 'most', 'inventive', 'comedy'], ['a', 'winning', 'animated', 'feature', 'that', 'has', 'something', 'for', 'everyone', 'on', 'the', 'age', 'spectrum']]\n",
      "number of tokens: 280092\n",
      "number of unique tokens: 22424\n",
      "number of documents: 14072\n"
     ]
    }
   ],
   "source": [
    "# This is a bit of text cleanup\n",
    "word_bag_list = []\n",
    "for doc in documents:\n",
    "    cleaned = doc.lower().replace('-', ' ')  # make lowercase and split hyphenated words in two\n",
    "    for c in string.punctuation:  # strip punctuation marks.\n",
    "        cleaned = cleaned.replace(c, '')\n",
    "    word_bag_list.append(wordpunct_tokenize(cleaned))\n",
    "\n",
    "# How do things look?\n",
    "print 'a few tokens:', word_bag_list[:3]\n",
    "\n",
    "# this flattens the nested lists into one big list for some stats\n",
    "token_list = []\n",
    "for tokens in word_bag_list:\n",
    "    token_list.extend(tokens)\n",
    "print 'number of tokens:', len(token_list)\n",
    "print 'number of unique tokens:', len(set(token_list))\n",
    "print 'number of documents:', len(word_bag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate the document frequency of all the unique tokens in the bags of words.\n",
    "\n",
    "df = Counter()  # initialize this dict-like thing.\n",
    "\n",
    "for doc in word_bag_list:\n",
    "    # FILL IN CODE\n",
    "    # count up the times words appear in INDIVIDUAL documents (not the total across all documents)\n",
    "    for something in something_else:  # edit this, obviously\n",
    "        # add one to the right key in df\n",
    "\n",
    "for token in df:\n",
    "    # normalize the counts by the number of documents (are you getting zeros? Think datatypes.)\n",
    "\n",
    "# this last line prints the 20 highest-scoring words and their scores\n",
    "df.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words\n",
    "\n",
    "Which words are likely to be stop words? The ones that show up in the most documents! These terms with the largest document frequency are the stopwords! The threshold above which you call something a stopword is up to you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf\n",
    "\n",
    "More interesting than stop-words is the tf-idf score. This tells us which words are most discriminative between documents. Words that occur a lot in one document but doesn't occur in many documents will tell you something special about the document:\n",
    "\n",
    "$$\n",
    "\\text{tf-idf} = tf \\cdot \\log{idf} = tf \\cdot \\log{1 \\over df} = tf \\cdot -\\log{df}\n",
    "$$\n",
    "\n",
    "recall that:\n",
    "\n",
    "$$\n",
    "\\log{x} = -\\log{1 \\over x}\n",
    "$$\n",
    "\n",
    "What are the most discriminative words in the first few documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate the term frequency of all the unique tokens in all of the bags of words.\n",
    "\n",
    "for doc in word_bag_list[:100]:\n",
    "    tf = Counter()  # initialize this dict-like thing.\n",
    "    tfidf = Counter()\n",
    "    \n",
    "    # FILL IN CODE\n",
    "\n",
    "    # calculate term frequencies\n",
    "    # this is similar to the document frequencies.\n",
    "    for something in something_else:\n",
    "\n",
    "    # calculate tf-idf scores\n",
    "    for token in tf:\n",
    "        tfidf[token] = # fill this in. you can use np.log().\n",
    "\n",
    "    # this prints most significant words in the document\n",
    "    print tfidf.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sci-Kit Learn\n",
    "\n",
    "Scikit-Learn comes with utilities to do these calculations for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_vec = TfidfVectorizer(stop_words='english')\n",
    "output = tfidf_vec.fit_transform(documents)\n",
    "print output.toarray()[20:30, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print tfidf_vec.get_stop_words()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
